# "UD" - is a special uniform design with 13 searches for the Gaussian SVM hyperparameters (C and sigma)
cat("fit SVM2 (uniform design search):\n")
tic()
s=list(search=mparheuristic("ksvm",5),method=c("holdoutorder",2/3))
svm2=fit(quality~.,w[H$tr,],model="ksvm",search=s)
toc()
print(svm2@object)
# analysis of the quality of the predictions on the test set:
pdt=predict(dt,w[H$ts,])
psvm=predict(svm,w[H$ts,])
psvm2=predict(svm2,w[H$ts,])
Y=w[H$ts,]$quality # target output variable
print("classification metrics:")
cat("class. metrics for dt:")
print(mmetric(Y,pdt,metric=c("ACC","AUC","macroF1","ACCLASS","AUCCLASS","F1")))
cat("class. metrics for svm:")
print(mmetric(Y,psvm,metric=c("ACC","AUC","macroF1","ACCLASS","AUCCLASS","F1")))
cat("class. metrics for svm2:")
print(mmetric(Y,psvm2,metric=c("ACC","AUC","macroF1","ACCLASS","AUCCLASS","F1")))
# examples of confusion matrices:
print("dt confusion matrix:")
# analysis of the quality of the predictions on the test set:
pdt=predict(dt,w[H$ts,])
psvm=predict(svm,w[H$ts,])
psvm2=predict(svm2,w[H$ts,])
Y=w[H$ts,]$quality # target output variable
print("classification metrics:")
cat("class. metrics for dt:")
print(mmetric(Y,pdt,metric=c("ACC","AUC","macroF1","ACCLASS","AUCCLASS","F1")))
cat("class. metrics for svm:")
print(mmetric(Y,psvm,metric=c("ACC","AUC","macroF1","ACCLASS","AUCCLASS","F1")))
cat("class. metrics for svm2:")
print(mmetric(Y,psvm2,metric=c("ACC","AUC","macroF1","ACCLASS","AUCCLASS","F1")))
# examples of confusion matrices:
print("dt confusion matrix:")
print(mmetric(Y,pdt,metric="CONF")$conf)
print("dt confusion matrix for class good:")
print(mmetric(Y,pdt,TC=3,metric="CONF")$conf)
print("dt confusion matrix for class good and D=0.7:") # more specific model
print(mmetric(Y,pdt,TC=3,D=0.6,metric="CONF")$conf)
print("dt confusion matrix for class good and D=0.3:") # more sensitive model
print(mmetric(Y,pdt,TC=3,D=0.3,metric="CONF")$conf)
# ROC curves:
# simple ROC curve for svm target class good (TC=3):
mgraph(Y,psvm,graph="ROC",TC=3,baseline=TRUE,leg="Good",Grid=10)
# more elaborated ROC curve:
L=vector("list",3) # needed because of mgraph multi ROC plot
testl=vector("list",1);testl[[1]]=Y # same TEST target for all 3 models
p1=vector("list",1);p1[[1]]=pdt
p2=vector("list",1);p2[[1]]=psvm
p3=vector("list",1);p3[[1]]=psvm2
L[[1]]=list(pred=p1,test=testl,runs=1)
L[[2]]=list(pred=p2,test=testl,runs=1)
L[[3]]=list(pred=p3,test=testl,runs=1)
auc1=round(mmetric(Y,pdt,metric="AUC",TC=3),digits=2)
auc2=round(mmetric(Y,psvm,metric="AUC",TC=3),digits=2)
auc3=round(mmetric(Y,psvm2,metric="AUC",TC=3),digits=2)
leg=c(paste("dt (AUC=",auc1,")",sep=""),paste("svm (AUC=",auc2,")",sep=""),paste("svm2 (AUC=",auc3,")",sep=""))
mgraph(L,graph="ROC",TC=3,baseline=TRUE,leg=list(pos="bottomright",leg=leg),main="ROC for Good",Grid=10)
# LIFT curves (useful for marketing tasks):
# simple LIFT curve for svm target class good (TC=3):
mgraph(Y,psvm,graph="LIFT",TC=3,baseline=TRUE,leg="Good",Grid=10)
# more elaborated LIFT curve:
# in this example, a new pdf file is created:
pdf("wine-lift.pdf",width=5,height=5) # diverts graph to PDF
L=vector("list",3) # needed because of mgraph multi ROC plot
testl=vector("list",1);testl[[1]]=Y # same TEST target for all 3 models
p1=vector("list",1);p1[[1]]=pdt
p2=vector("list",1);p2[[1]]=psvm
p3=vector("list",1);p3[[1]]=psvm2
L[[1]]=list(pred=p1,test=testl,runs=1)
L[[2]]=list(pred=p2,test=testl,runs=1)
L[[3]]=list(pred=p3,test=testl,runs=1)
auc1=round(mmetric(Y,pdt,metric="ALIFT",TC=3),digits=2)
auc2=round(mmetric(Y,psvm,metric="ALIFT",TC=3),digits=2)
auc3=round(mmetric(Y,psvm2,metric="ALIFT",TC=3),digits=2)
leg=c(paste("dt (ALIFT=",auc1,")",sep=""),paste("svm (ALIFT=",auc2,")",sep=""),paste("svm2 (ALIFT=",auc3,")",sep=""))
mgraph(L,graph="LIFT",TC=3,baseline=TRUE,leg=list(pos=c(0.4,0.3),leg=leg),main="LIFT for Good",Grid=10)
dev.off() # closes PDF
# AUTOML classification demo
library(rminer)
library(tictoc)
# classification demo, using wine quality from UCI:
path="https://archive.ics.uci.edu/ml/machine-learning-databases/wine-quality/"
file=paste(path,"winequality-white.csv",sep="")
w=read.table(file,sep=";",header=TRUE)
w$quality=cut(w$quality,c(0,5.5,6.5,10),c("bad","medium","good"))
# to speed up results, lets work on a smaller dataset sample size: 1000 rows
s=1:1000
w=w[s,]
# lets assume a simple time ordered holdout split:
H=holdout(w$quality,ratio=2/3,mode="order")
print("### automl (faster) ###")
# use of auto-ml, mode 1 (faster): "automl"
inputs=ncol(w)-1 # number of inputs, needed for random forest
metric="macroF1"
sm=mparheuristic(model="automl",task="prob",inputs=inputs)
# internal validation method (used within the training data)
imethod=c("holdoutorder",2/3) # internal validation method
search=list(search=sm,smethod="auto",method=imethod,metric=metric,convex=0)
# external validation method:
emethod=holdout(w$quality,2/3,mode="order") # object with tr and ts indexes
# execution of 1 run of the fast automl1:
print("automl1 fit:")
tic()
M=fit(quality~.,data=w[emethod$tr,],model="auto",search=search,fdebug=TRUE)
# emethod$ts - test data rows
toc()
P=predict(M,w[emethod$ts,])
# show leaderboard:
cat("> leaderboard models:",M@mpar$LB$model,"\n")
cat(">  validation values:",round(M@mpar$LB$eval,4),"\n")
cat("best model is:",M@model,"\n")
cat("test set ",metric,"=",round(mmetric(w$quality[emethod$ts],P,metric=metric),2),"\n")
print("### automl3 (might provide better results, requires more computation) ###")
# use of auto-ml, mode 3: "automl3"
sm3=mparheuristic(model="automl3",task="prob",inputs=inputs)
# internal validation method (used within the training data)
search3=list(search=sm3,smethod="auto",method=imethod,metric=metric,convex=0)
# execution of 1 run of the fast automl1:
print("automl3 fit:")
tic()
M3=fit(quality~.,data=w[emethod$tr,],model="auto",search=search3,fdebug=TRUE)
toc()
# emethod$ts - test data rows
P3=predict(M3,w[emethod$ts,])
# show leaderboard:
cat("> leaderboard models:",M3@mpar$LB$model,"\n")
# you need to install this package: install.packages("genalg")
library(genalg)
# string is a vector of binary 0 or 1 values
# rbga.bin always performs a minimization task, thus the sum of bits ( sum(string) )is transformed
# into a minimization task using: K - eval(S):
evaluate=function(string=c())
{ return ( length(string) - sum(string)) }
nbits=24
iter=100
cat("rbga.bin sum of bits demo (nbits=",nbits,"):\n")
# genetic algorithm for a binary representation with a size of 24 bits, each bit is 0 or 1:
bga= rbga.bin(size=nbits,popSize=20,mutationChance=0.01,zeroToOneRatio=1,elitism=0,evalFunc=evaluate,iter=iter)
# visual example of the evolution of the rbga.bin optimization
plot(bga)
bindex=which.min(bga$evaluations)
# string is a vector of binary 0 or 1 values
# rbga.bin always performs a minimization task, thus the sum of bits ( sum(string) )is transformed
# into a minimization task using: K - eval(S):
evaluate=function(string=c())
{ return ( length(string) - sum(string)) }
nbits=24
iter=100
cat("rbga.bin sum of bits demo (nbits=",nbits,"):\n")
# genetic algorithm for a binary representation with a size of 24 bits, each bit is 0 or 1:
bga= rbga.bin(size=nbits,popSize=20,mutationChance=0.01,zeroToOneRatio=1,elitism=0,evalFunc=evaluate,iter=iter)
# visual example of the evolution of the rbga.bin optimization
plot(bga)
bindex=which.min(bga$evaluations)
cat("best solution:",bga$population[bindex,],"evaluation function",bga$evaluations[bindex],"\n")
# you need to install this package:
library(genalg)
# definition of the famous rastrigin function
# x is a vector with D real values.
rastrigin=function(x=c()) { return (sum(x^2-10*cos(2*pi*x)+10))}
monitor=function(obj) {
xlim = c(-LIM,LIM) # x-axis limits
ylim = c(-LIM,LIM) # y-axis limits
COL=paste("gray",80-ITER*4,sep="") # set the points color depending on ITER
for(i in 1:nrow(obj$population))
cat(obj$population[i,]," fit:",obj$evaluations[i],"\n") # show in console all individual values and fitness
PMIN=which.min(obj$evaluations) # show which point provides the best (lowest) value
cat("PMIN:",PMIN,"\n")
plot(obj$population, xlim=xlim, ylim=ylim, xlab="x", ylab="y",pch=19,cex=0.5,col=COL) # plot all points
#plot(obj$population, xlim=xlim, ylim=ylim, xlab="x", ylab="y",pch=19,cex=sizepop(obj$evaluations));
text(obj$population[PMIN,1],obj$population[PMIN,2],"min") # put the label "min" near the best point
cat("-- generation:",ITER,"(press enter)\n");readLines(n=1) # wait for user to press enter
ITER<<-ITER+1 # global variable ITER increase (outside this function, ITER is valid)
}
LIM=5.12 # used for lower and upper bounds, global variable
# first experiment, popSize=20:
ITER<<-1 # global variable assignment (because monitor only receives obj as input)
rga=rbga(c(-LIM,-LIM),c(LIM,LIM),popSize=20,mutationChance=0.33,elitism=1,evalFunc=rastrigin,monitorFunc=monitor,iter=20)
# you need to install these packages:
library(genalg)
library(DEoptim)
library(pso)
# definition of the famous rastrigin function
# x is a vector with D real values.
rastrigin=function(x=c()) { return (sum(x^2-10*cos(2*pi*x)+10))}
# global variables, defined outside functions: ------------------------
# lets set D to 30 (good benchmark, challenging):
D=30
LIM=5.12 # used for lower and upper bounds, global variable
# some parameters that will be equal for all methods:
popSize=100 # population size
iter=100 # maximum number of iterations
report=10 # report progress every 10 iterations
lower=rep(-LIM,D) # lower bounds for all D values
upper=rep(LIM,D)  # upper bounds for all D values
## show best:
showbest=function(method,par,eval)
{ cat("method:",method,"\n > par:",par,"\n > eval:",eval,"\n") }
# Genetic Algorithm optimization: ------------------------------
ITER<<- 1 # global variable with number of rbga iterations
# monitoring function:
traceGA=function(obj)
{ if((ITER %% report)==0) # show progress every report iterations
{ PMIN=which.min(obj$evaluations)
cat("iter:",ITER," eval:",obj$evaluations[PMIN],"\n")
}
ITER<<-ITER+1
}
# call to rbga: Genetic Algorithm:
rga=rbga(lower,upper,popSize=popSize,evalFunc=rastrigin,iter=iter,monitor=traceGA)
# get the best solution:
# note: the way to get the best solution and evaluation depends on the implementation of the method and thus
# it can be different from method to method:
PMIN=which.min(rga$evaluations)
showbest("rbga",rga$population[PMIN,],rga$evaluations[PMIN])
# Differential Evolution Optimization: -------------------------
de=DEoptim(fn=rastrigin,lower=lower,upper=upper,DEoptim.control(NP=popSize,itermax=iter,trace=report))
# get the best solution:
# note: the way to get the best solution and evaluation depends on the implementation of the method and thus
# it can be different from method to method:
showbest("DEoptim",de$optim$bestmem,de$optim$bestval)
# Particle Swarm Optimization: ---------------------------------
# note: par needs to be vector with the size of D, in this case I am using lower, but upper could also be used.
# the values of par are not used.
ps=psoptim(par=lower,fn=rastrigin,lower=lower,upper=upper,control=list(trace=1,REPORT=report,maxit=iter,s=popSize))
# get the best solution:
# note: the way to get the best solution and evaluation depends on the implementation of the method and thus
# it can be different from method to method:
showbest("psoptim",ps$par,ps$value)
# you need to install these packages:
library(genalg)
library(DEoptim)
library(pso)
# definition of the famous rastrigin function
# x is a vector with D real values.
rastrigin=function(x=c()) { return (sum(x^2-10*cos(2*pi*x)+10))}
# global variables, defined outside functions: ------------------------
# lets set D to 30 (good benchmark, challenging):
D=30
LIM=5.12 # used for lower and upper bounds, global variable
library(rminer)
library(forecast)
# Loop through columns 4 to 7
for (i in 4:7) {
cat("Departamento:", colnames(data)[i], "\n")
# read the time series into object S
S <- read.table("C:/Users/joana/OneDrive/Ambiente de Trabalho/uni/4 ano/2 semestre/TIAPOSE/projeto/walmart.csv", header = TRUE, sep = ",")[,i]
NPRED <- 4 # number of predictions
srange <- diff(range(S)) # calculates the difference between the maximum and minimum values in S
# show S and some statistics:
plot(S, type="l", col="blue")
acf(S) # autocorrelation plot
mpause() # rminer function, requires user to press enter
# creates a dataframe
# each row contains past observations
# each column represents the value of the time series
# uses rolling window
D <- CasesSeries(S, c(1:4))
print(summary(D))
N <- nrow(D) # number of D examples
NTR <- N - NPRED # calculates the number of examples in the training set by subtracting the predictions
# training data, excluding NPRED rows
TR <- 1:NTR
# test data
TS <- (NTR + 1):N
# fit a random forest
RF <- fit(y ~ ., D[TR,], model = "randomForest", search = "heuristic")
# 1-ahead predictions:
LTS <- length(TS) # length of the test set
cat("Predictions (from 1-ahead to ", LTS, "-ahead):\n", sep="")
START <- nrow(D) - LTS + 1 # START is the row from D of the first test example
PRF <- lforecast(RF, D, start = START, horizon = LTS)
# store the output target into object Y
Y <- D[TS,]$y # real observed values
cat("RF predictions:\n", colnames(data)[i], ":\n")
print(PRF)
cat("MAE:", mmetric(Y, PRF, metric = "MAE"), "\n")
cat("NMAE:", mmetric(Y, PRF, metric = "NMAE", val = srange), "\n")
cat("RMSE:", mmetric(Y, PRF, metric = "RMSE"), "\n")
cat("RRSE:", mmetric(Y, PRF, metric = "RRSE"), "\n")
cat("R2:", mmetric(Y, PRF, metric = "R22"), "\n") # press R measure
# graph: REG - simple Regression Plot
nmae <- mmetric(Y, PRF, metric = "NMAE", val = srange)
print("Gráfico RF")
main <- paste("Random Forest pred. (NMAE=", round(nmae, digits = 0), ")", sep="")
mgraph(Y, PRF, main = main, graph = "REG", Grid = 10, lty = 1, col = c("black", "blue"), leg = list(pos = "topright", leg = c("target", "predictions")))
# Pause to view the plot before proceeding to the next department
mpause()
}
View(emethod)
View(emethod)
setwd("C:/Users/joaom/Documents/ProjetoTIAPOSE")
# Função Monte Carlo
source("blind.R")
source("montecarlo.R")
eval_fn <- function(s, previsoes_vendas) {
s <- round(s)
vendas_previstas <- previsoes_vendas
hired_workers <- matrix(s[1:12], nrow = 3, ncol = 4, byrow = TRUE)
custo_junior <- 6000
custo_normal <- 8000
custo_senior <- 9750
total_cost_workers <- sum(
hired_workers[1, ] * custo_junior,
hired_workers[2, ] * custo_normal,
hired_workers[3, ] * custo_senior
)
product_orders <- matrix(s[13:28], nrow = 4, ncol = 4, byrow = TRUE)
custo_encomenda <- c(6, 8, 9, 11)
total_cost_orders <- sum(
product_orders[, 1] * custo_encomenda[1],
product_orders[, 2] * custo_encomenda[2],
product_orders[, 3] * custo_encomenda[3],
product_orders[, 4] * custo_encomenda[4]
)
vendas_previstas <- previsoes_vendas
capacidade_junior <- 4000
capacidade_normal <- 7000
capacidade_senior <- 9500
worker_max_support_per_dept <- apply(hired_workers, 2, function(col) {
col[1] * capacidade_junior + col[2] * capacidade_normal + col[3] * capacidade_senior
})
calcularVendasEfetivas <- function(vendas_previstas, product_orders, worker_max_support_per_dept) {
vendasEfetivas <- matrix(0, nrow = nrow(vendas_previstas), ncol = ncol(vendas_previstas))
for (i in 1:nrow(vendas_previstas)) {
for (j in 1:ncol(vendas_previstas)) {
if (vendas_previstas[i, j] <= product_orders[i, j] & vendas_previstas[i, j] <= worker_max_support_per_dept[j]) {
vendasEfetivas[i, j] <- vendas_previstas[i, j]
} else if (i > 1 && vendas_previstas[i - 1, j] <= product_orders[i - 1, j]) {
vendasEfetivas[i, j] <- product_orders[i-1, j] - vendas_previstas[i - 1, j]
} else {
vendasEfetivas[i, j] <- min(product_orders[i, j], worker_max_support_per_dept[j])
}
}
}
return(vendasEfetivas)
}
sales <- calcularVendasEfetivas(vendas_previstas, product_orders, worker_max_support_per_dept)
valor_venda <- c(8, 10, 12, 16)
sales_in_usd <- matrix(0, nrow = 4, ncol = 4)
for (i in 1:4) {
sales_in_usd[, i] <- sales[, i] * valor_venda[i]
}
total_revenue_sales <- sum(sales_in_usd)
stock <- matrix(0, nrow = 4, ncol = 4)
for (s in 1:4) {
for (d in 1:4) {
if (s == 1) {
stock[s, d] <- product_orders[s, d] - sales[s, d]
} else {
stock[s, d] <- stock[s - 1, d] + product_orders[s, d] - sales[s, d]
}
}
}
stock_cost_per_product <- c(3, 5, 6, 8)
stock_costs <- matrix(0, nrow = 4, ncol = 4)
for (i in 1:4) {
stock_costs[, i] <- stock[, i] * stock_cost_per_product[i]
}
total_stock_cost <- sum(stock_costs)
total_costs <- total_cost_workers + total_cost_orders + total_stock_cost
month_profit <- total_revenue_sales - total_costs
return(list(
month_profit = month_profit,
total_cost_workers = total_cost_workers,
total_cost_orders = total_cost_orders,
total_stock_cost = total_stock_cost,
total_revenue_sales = total_revenue_sales,
sales = sales
stock = stock
# Função Monte Carlo
source("blind.R")
source("montecarlo.R")
eval_fn <- function(s, previsoes_vendas) {
s <- round(s)
vendas_previstas <- previsoes_vendas
hired_workers <- matrix(s[1:12], nrow = 3, ncol = 4, byrow = TRUE)
custo_junior <- 6000
custo_normal <- 8000
custo_senior <- 9750
total_cost_workers <- sum(
hired_workers[1, ] * custo_junior,
hired_workers[2, ] * custo_normal,
hired_workers[3, ] * custo_senior
)
product_orders <- matrix(s[13:28], nrow = 4, ncol = 4, byrow = TRUE)
custo_encomenda <- c(6, 8, 9, 11)
total_cost_orders <- sum(
product_orders[, 1] * custo_encomenda[1],
product_orders[, 2] * custo_encomenda[2],
product_orders[, 3] * custo_encomenda[3],
product_orders[, 4] * custo_encomenda[4]
)
vendas_previstas <- previsoes_vendas
capacidade_junior <- 4000
capacidade_normal <- 7000
capacidade_senior <- 9500
worker_max_support_per_dept <- apply(hired_workers, 2, function(col) {
col[1] * capacidade_junior + col[2] * capacidade_normal + col[3] * capacidade_senior
})
calcularVendasEfetivas <- function(vendas_previstas, product_orders, worker_max_support_per_dept) {
vendasEfetivas <- matrix(0, nrow = nrow(vendas_previstas), ncol = ncol(vendas_previstas))
for (i in 1:nrow(vendas_previstas)) {
for (j in 1:ncol(vendas_previstas)) {
if (vendas_previstas[i, j] <= product_orders[i, j] & vendas_previstas[i, j] <= worker_max_support_per_dept[j]) {
vendasEfetivas[i, j] <- vendas_previstas[i, j]
} else if (i > 1 && vendas_previstas[i - 1, j] <= product_orders[i - 1, j]) {
vendasEfetivas[i, j] <- product_orders[i-1, j] - vendas_previstas[i - 1, j]
} else {
vendasEfetivas[i, j] <- min(product_orders[i, j], worker_max_support_per_dept[j])
}
}
}
return(vendasEfetivas)
}
sales <- calcularVendasEfetivas(vendas_previstas, product_orders, worker_max_support_per_dept)
valor_venda <- c(8, 10, 12, 16)
sales_in_usd <- matrix(0, nrow = 4, ncol = 4)
for (i in 1:4) {
sales_in_usd[, i] <- sales[, i] * valor_venda[i]
}
total_revenue_sales <- sum(sales_in_usd)
stock <- matrix(0, nrow = 4, ncol = 4)
for (s in 1:4) {
for (d in 1:4) {
if (s == 1) {
stock[s, d] <- product_orders[s, d] - sales[s, d]
} else {
stock[s, d] <- stock[s - 1, d] + product_orders[s, d] - sales[s, d]
}
}
}
stock_cost_per_product <- c(3, 5, 6, 8)
stock_costs <- matrix(0, nrow = 4, ncol = 4)
for (i in 1:4) {
stock_costs[, i] <- stock[, i] * stock_cost_per_product[i]
}
total_stock_cost <- sum(stock_costs)
total_costs <- total_cost_workers + total_cost_orders + total_stock_cost
month_profit <- total_revenue_sales - total_costs
return(list(
month_profit = month_profit,
total_cost_workers = total_cost_workers,
total_cost_orders = total_cost_orders,
total_stock_cost = total_stock_cost,
total_revenue_sales = total_revenue_sales,
sales = sales,
stock = stock
))
}
calcular_upper <- function(previsoes_vendas) {
tipo <- c(4000, 7000, 9500)
actual_sales_by_dep <- colSums(previsoes_vendas)
upper <- numeric(28)
for (i in 1:ncol(previsoes_vendas)) {
max_sales <- max(previsoes_vendas[[i]])
if (max_sales != 0) {
for (j in 1:length(tipo)) {
upper[(i - 1) * 3 + j] <- ceiling(max_sales / tipo[j])
}
}
}
for (i in 1:ncol(previsoes_vendas)) {
for (j in 1:4) {
if (j == 1) {
upper[12 + (i - 1) * 4 + j] <- sum(previsoes_vendas[, i])
} else if (j == 2) {
upper[12 + (i - 1) * 4 + j] <- sum(previsoes_vendas[, i][-1])
} else if (j == 3) {
upper[12 + (i - 1) * 4 + j] <- sum(previsoes_vendas[, i][-c(1:2)])
} else {
upper[12 + (i - 1) * 4 + j] <- previsoes_vendas[nrow(previsoes_vendas), i]
}
}
}
return(upper)
}
montecarlo_optimizar <- function(previsoes_vendas) {
lower <- rep(0, 28)
upper <- calcular_upper(previsoes_vendas)
N <- 75000
resultado <- mcsearch(fn = function(s) eval_fn(s, previsoes_vendas), lower = lower, upper = upper, N = N, type = "max")
melhor_solucao <- round(resultado$sol)
melhor_lucro <- resultado$eval
eval_result <- eval_fn(melhor_solucao, previsoes_vendas)
return(list(
solucao = melhor_solucao,
lucro = melhor_lucro,
total_cost_workers = eval_result$total_cost_workers,
total_cost_orders = eval_result$total_cost_orders,
total_stock_cost = eval_result$total_stock_cost,
total_revenue_sales = eval_result$total_revenue_sales,
sales = eval_result$sales,
stock = eval_result$stock
))
}
previsoes_vendas <- matrix(
c(43096.36,	145650.33,	62598.95,	131354.77,
41404.49, 136850.91,	62169.49,	126786.53,
42302.73,	127778.38,	61632.76,	123543.41,
43847.65,	137070.04,	63591.24,	129183.80),
nrow = 4, ncol = 4, byrow = TRUE
)
# Supondo que 's' seja um vetor de 28 elementos adequado para a função eval_fn
# Aqui vou usar valores fictícios, mas você pode substituir pelos valores adequados
s <- c(0, 0, 0, 11, 6, 0, 0, 4, 0, 9, 4, 2, 0, 47263, 23020, 43848, 93725, 0, 0, 0, 0, 0, 28400, 63591, 0, 0, 0, 86677)
# Avaliando a função com os valores fornecidos
resultados <- eval_fn(s, previsoes_vendas)
# Imprimindo os resultados
print(resultados)
